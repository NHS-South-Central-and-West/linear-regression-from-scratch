[
  {
    "objectID": "index.html#what-is-regression",
    "href": "index.html#what-is-regression",
    "title": "Linear Regression from Scratch",
    "section": "What is Regression?",
    "text": "What is Regression?\n\nRegression models predict an outcome variable from a set of predictors. This can take the form of straight lines fit through the data, to many non-linear generalisations (Gelman, Hill, and Vehtari 2021).\nRegression can be used to describe the association between variables, estimate the effect that a treatment has on an outcome, or predict how an outcome will change in response to changes in the predictor variables.\nIt is the foundation for many advanced statistical methods, and it is one of the most important tools available to an analyst seeking to use a sample to make inferences or predictions about the population (Rowntree 2018).\nWhile powerful, the method itself is (relatively) simple, when boiled down to its component parts."
  },
  {
    "objectID": "index.html#its-all-about-the-covariance",
    "href": "index.html#its-all-about-the-covariance",
    "title": "Linear Regression from Scratch",
    "section": "It’s All About the (Co)Variance",
    "text": "It’s All About the (Co)Variance\n\nIt is possible to make predictions or estimate effects between quantities of interest by analysing how they vary, both independently and together.\nThe extent to which the predictor variable, \\(X\\), and the outcome, \\(Y\\), move together is called “covariance”.\n\nA positive covariance indicates that the value of \\(Y\\) increases when \\(X\\) increases, and a negative covariance indicates that \\(Y\\) decreases when \\(X\\) decreases.\n\nRegression fits a line through data that estimate how the outcome variable changes when the value of the predictor variable(s) change. Finding the line that does the most effective job of passing through the data helps describe the relationship between the variables, or predict the outcome from the predictors.\nThis doesn’t sound like much, but it’s a surprisingly powerful idea."
  },
  {
    "objectID": "index.html#fitting-a-line-through-data",
    "href": "index.html#fitting-a-line-through-data",
    "title": "Linear Regression from Scratch",
    "section": "Fitting a Line Through Data",
    "text": "Fitting a Line Through Data\n\nAt it’s most basic conceptual level, regression is just finding the line of best fit through the data.\n\nThe complexity comes in the need to use precise, unbiased methods to find the “best” fit.\n\nThere are various “estimators” that can be used to fit a straight line to data, but the most common method is called ordinary least squares (OLS).\nOLS finds the line of best fit by minimising the total distance between the line and all observed values. The distance between the line, which represents the model’s fitted values (or the predicted value of \\(Y\\) given \\(X\\)) and each observed value in the data is known as the residual.\nAll paths lead to minimising the residuals."
  },
  {
    "objectID": "index.html#visualising-the-outcome-variance",
    "href": "index.html#visualising-the-outcome-variance",
    "title": "Linear Regression from Scratch",
    "section": "Visualising the Outcome Variance",
    "text": "Visualising the Outcome Variance\n\n\ndf |&gt; \n  ggplot(aes(x = exam_score)) +\n  geom_histogram(binwidth = 1) +\n  labs(x = \"Exam Score\", y = NULL)"
  },
  {
    "objectID": "index.html#visualising-the-outcome-variance-1",
    "href": "index.html#visualising-the-outcome-variance-1",
    "title": "Linear Regression from Scratch",
    "section": "Visualising the Outcome Variance",
    "text": "Visualising the Outcome Variance\n\n\ndf &lt;- df |&gt; filter(exam_score &lt; 80)\n  \ndf |&gt; \n  ggplot(aes(x = exam_score)) +\n  geom_histogram(binwidth = 1) +\n  labs(x = \"Exam Score\", y = NULL)"
  },
  {
    "objectID": "index.html#visualising-the-predictor-variance",
    "href": "index.html#visualising-the-predictor-variance",
    "title": "Linear Regression from Scratch",
    "section": "Visualising the Predictor Variance",
    "text": "Visualising the Predictor Variance\n\n\ndf |&gt; \n  ggplot(aes(x = hours_studied)) +\n  geom_histogram(binwidth = 1) +\n  labs(x = \"Hours Studied\", y = NULL)"
  },
  {
    "objectID": "index.html#visualising-their-covariance",
    "href": "index.html#visualising-their-covariance",
    "title": "Linear Regression from Scratch",
    "section": "Visualising their Covariance",
    "text": "Visualising their Covariance\n\n\ndf |&gt; \n  ggplot(aes(x = hours_studied, y = exam_score)) +\n  geom_point(shape = 21, fill = \"white\", size = 1.5, stroke = 1) +\n  labs(x = \"Hours Studied\", y = \"Exam Score\")"
  },
  {
    "objectID": "index.html#fitting-a-regression-line",
    "href": "index.html#fitting-a-regression-line",
    "title": "Linear Regression from Scratch",
    "section": "Fitting a Regression Line",
    "text": "Fitting a Regression Line\n\n\ndf |&gt; \n  ggplot(aes(x = hours_studied, y = exam_score)) +\n  geom_point(shape = 21, fill = \"white\", size = 1.5, stroke = 1) +\n  geom_smooth(\n    method = lm, se = FALSE, linewidth = 1, colour = \"#005EB8\"\n    ) +\n  labs(x = \"Hours Studied\", y = \"Exam Score\")"
  },
  {
    "objectID": "index.html#finding-the-line-of-best-fit",
    "href": "index.html#finding-the-line-of-best-fit",
    "title": "Linear Regression from Scratch",
    "section": "Finding the Line of “Best Fit”",
    "text": "Finding the Line of “Best Fit”\n\n\nmodel &lt;- lm(exam_score ~ hours_studied, data = df)\n\ndf &lt;-\n  df |&gt; \n  mutate(\n    fitted = model$fitted.values, \n    residual = model$residuals\n    )\n\ndf |&gt; \n  select(hours_studied, exam_score, fitted, residual) |&gt; \n  janitor::clean_names(case = \"title\") |&gt; \n  slice_sample(n = 10) |&gt; \n  gt() |&gt; \n  fmt_number(columns = c(Fitted, Residual), decimals = 2) |&gt; \n  cols_align(align = \"center\", columns = everything())\n\n\n\n\n\n\n\n\nHours Studied\nExam Score\nFitted\nResidual\n\n\n\n\n24\n67\n68.25\n−1.25\n\n\n26\n69\n68.83\n0.17\n\n\n35\n69\n71.46\n−2.46\n\n\n22\n71\n67.66\n3.34\n\n\n24\n70\n68.25\n1.75\n\n\n26\n72\n68.83\n3.17\n\n\n13\n63\n65.03\n−2.03\n\n\n24\n68\n68.25\n−0.25\n\n\n19\n64\n66.79\n−2.79\n\n\n32\n66\n70.58\n−4.58"
  },
  {
    "objectID": "index.html#finding-the-line-of-best-fit-1",
    "href": "index.html#finding-the-line-of-best-fit-1",
    "title": "Linear Regression from Scratch",
    "section": "Finding the Line of “Best Fit”",
    "text": "Finding the Line of “Best Fit”\n\n\ndf |&gt; \n  ggplot(aes(x = hours_studied, y = exam_score)) +\n  geom_segment(\n    aes(\n      x = hours_studied, y= fitted, \n      xend = hours_studied, yend = fitted + residual\n      ), \n    linewidth = 1, color = \"#ED8B00\"\n    ) +\n  geom_point(shape = 21, fill = \"white\", size = 1.5, stroke = 1) +\n  geom_line(\n    aes(x = hours_studied, y = fitted), \n    linewidth = 1, colour = \"#005EB8\"\n    ) +\n  labs(x = \"Hours Studied\", y = \"Exam Score\")"
  },
  {
    "objectID": "index.html#finding-the-line-of-best-fit-2",
    "href": "index.html#finding-the-line-of-best-fit-2",
    "title": "Linear Regression from Scratch",
    "section": "Finding the Line of “Best Fit”",
    "text": "Finding the Line of “Best Fit”\n\n\ndf |&gt;\n  slice_sample(n = 10) |&gt; \n  ggplot(aes(x = hours_studied, y = exam_score)) +\n  geom_segment(\n    aes(\n      x = hours_studied, y= fitted, \n      xend = hours_studied, yend = fitted + residual\n      ), \n    linewidth = 1, color = \"#ED8B00\"\n    ) +\n  geom_point(shape = 21, fill = \"white\", size = 1.5, stroke = 1) +\n  geom_line(\n    aes(x = hours_studied, y = fitted), \n    linewidth = 1, colour = \"#005EB8\"\n    ) +\n  labs(x = \"Hours Studied\", y = \"Exam Score\")"
  },
  {
    "objectID": "index.html#finding-the-line-of-best-fit-3",
    "href": "index.html#finding-the-line-of-best-fit-3",
    "title": "Linear Regression from Scratch",
    "section": "Finding the Line of “Best Fit”",
    "text": "Finding the Line of “Best Fit”\n\n\ndf |&gt;\n  slice_sample(n = 100) |&gt; \n  ggplot(aes(x = hours_studied, y = exam_score)) +\n  geom_segment(\n    aes(\n      x = hours_studied, y= fitted, \n      xend = hours_studied, yend = fitted + residual\n      ), \n    linewidth = 1, color = \"#ED8B00\"\n    ) +\n  geom_point(shape = 21, fill = \"white\", size = 1.5, stroke = 1) +\n  geom_line(\n    aes(x = hours_studied, y = fitted), \n    linewidth = 1, colour = \"#005EB8\"\n    ) +\n  labs(x = \"Hours Studied\", y = \"Exam Score\")"
  },
  {
    "objectID": "index.html#finding-the-line-of-best-fit-4",
    "href": "index.html#finding-the-line-of-best-fit-4",
    "title": "Linear Regression from Scratch",
    "section": "Finding the Line of “Best Fit”",
    "text": "Finding the Line of “Best Fit”\n\n\ndf |&gt;\n  ggplot(aes(x = hours_studied, y = exam_score)) +\n  geom_segment(\n    aes(\n      x = hours_studied, y= fitted, \n      xend = hours_studied, yend = fitted + residual\n      ), \n    linewidth = 1, color = \"#ED8B00\"\n    ) +\n  geom_point(shape = 21, fill = \"white\", size = 1.5, stroke = 1) +\n  geom_line(\n    aes(x = hours_studied, y = fitted), \n    linewidth = 1, colour = \"#005EB8\"\n    ) +\n  labs(x = \"Hours Studied\", y = \"Exam Score\")"
  },
  {
    "objectID": "index.html#minimising-the-residuals",
    "href": "index.html#minimising-the-residuals",
    "title": "Linear Regression from Scratch",
    "section": "Minimising the Residuals",
    "text": "Minimising the Residuals\n\nThe least squares estimator takes the residual value of all of the regression’s predictions, squares them, and then sums them to get a single value that captures how well the model fits to the data.\n\nSquaring the value of each residual ensures sure that all values are positive, so that they don’t cancel each other out, and gives greater weight to larger residuals, making sure that the fitted line accounts for values that are far away from the mean. This value is known as the residual sum of squares (RSS).\n\\(\\text{RSS} = \\sum_{i=1}^n (y_i - (\\beta_0 + \\beta_1 x_i))^2\\)\n\nFinding the line that minimises the RSS produces an unbiased linear estimator (assuming that the regression assumptions are all met), or the line of best fit!"
  },
  {
    "objectID": "index.html#solve-for-beta_0-and-beta_1-minimise-rss",
    "href": "index.html#solve-for-beta_0-and-beta_1-minimise-rss",
    "title": "Linear Regression from Scratch",
    "section": "Solve for \\(\\beta_0\\) and \\(\\beta_1\\); Minimise RSS",
    "text": "Solve for \\(\\beta_0\\) and \\(\\beta_1\\); Minimise RSS\n\n\\(\\text{Residual Sum of Squares (RSS)} = \\sum_{i=1}^n \\left( y_i - (\\beta_0 + \\beta_1 x_i) \\right)^2\\)\nMinimising RSS gives us the line that best fits the data, but we don’t know what \\(\\beta_0\\) or \\(\\beta_1\\) are!\nMinimize RSS (Solve for \\(\\beta_0\\), then \\(\\beta_1\\))\n\n\n\n\n\\(\\frac{\\partial}{\\partial \\beta_0} \\text{RSS} = \\frac{\\partial}{\\partial \\beta_0} \\sum_{i=1}^n \\left( y_i - (\\beta_0 + \\beta_1 x_i) \\right)^2\\)\n\\(\\frac{\\partial}{\\partial \\beta_0} \\text{RSS} = -2 \\sum_{i=1}^n \\left( y_i - (\\beta_0 + \\beta_1 x_i) \\right)\\)\n\\(\\sum_{i=1}^n \\left( y_i - (\\beta_0 + \\beta_1 x_i) \\right) = 0\\)\n\\(n \\beta_0 + \\beta_1 \\sum_{i=1}^n x_i = \\sum_{i=1}^n y_i\\)\n\\(\\beta_0 = \\bar{y} - \\beta_1 \\bar{x}\\)\n\n\n\n\\(\\frac{\\partial}{\\partial \\beta_1} \\text{RSS} = \\frac{\\partial}{\\partial \\beta_1} \\sum_{i=1}^n \\left( y_i - (\\beta_0 + \\beta_1 x_i) \\right)^2\\)\n\\(\\frac{\\partial}{\\partial \\beta_1} \\text{RSS} = -2 \\sum_{i=1}^n x_i \\left( y_i - (\\beta_0 + \\beta_1 x_i) \\right)\\)\n\\(\\sum_{i=1}^n x_i \\left( y_i - (\\beta_0 + \\beta_1 x_i) \\right) = 0\\)\n\\(\\beta_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\)\n\n\n\nFinal Coefficients - \\(\\beta_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\frac{\\text{Cov}(X, Y)}{\\text{Var}(X)}\\) ; \\(\\beta_0 = \\bar{y} - \\beta_1 \\bar{x}\\)"
  },
  {
    "objectID": "index.html#the-most-perfect-little-equation",
    "href": "index.html#the-most-perfect-little-equation",
    "title": "Linear Regression from Scratch",
    "section": "The Most Perfect Little Equation",
    "text": "The Most Perfect Little Equation\n\nThe formula for a simple linear regression model, predicting \\(Y\\) with one predictor \\(X\\), is as follows:\n\n\\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\]\n\nThis breaks the problem down into three main components, and estimates two parameters:\n\n\\(\\beta_1\\) - The slope, estimating the effect that \\(X\\) has on the outcome, \\(Y\\).\n\\(\\beta_0\\) - The intercept, estimating the average value of \\(Y\\) when \\(X = 0\\).\n\\(\\epsilon\\) - The error term (the unexplained variance), capturing the remaining variance in the outcome \\(Y\\) that is not explained by the rest of the model."
  },
  {
    "objectID": "index.html#calculating-the-regression-slope-beta_1",
    "href": "index.html#calculating-the-regression-slope-beta_1",
    "title": "Linear Regression from Scratch",
    "section": "Calculating the Regression Slope (\\(\\beta_1\\))",
    "text": "Calculating the Regression Slope (\\(\\beta_1\\))\n\\[\\beta_1 = \\frac{\\text{Cov}(X, Y)}{\\text{Var}(X)} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2} \\]"
  },
  {
    "objectID": "index.html#calculating-the-regression-intercept-beta_0",
    "href": "index.html#calculating-the-regression-intercept-beta_0",
    "title": "Linear Regression from Scratch",
    "section": "Calculating the Regression Intercept (\\(\\beta_0\\))",
    "text": "Calculating the Regression Intercept (\\(\\beta_0\\))\n\\[\\beta_0 = \\bar{y} - \\beta_1 \\bar{x} \\]"
  },
  {
    "objectID": "index.html#predicting-the-outcome-haty",
    "href": "index.html#predicting-the-outcome-haty",
    "title": "Linear Regression from Scratch",
    "section": "Predicting the Outcome (\\(\\hat{y}\\))",
    "text": "Predicting the Outcome (\\(\\hat{y}\\))\n\\[\\hat{y}_i = \\beta_0 + \\beta_1 x_i \\]"
  },
  {
    "objectID": "index.html#its-just-a-regression-with-more-variables",
    "href": "index.html#its-just-a-regression-with-more-variables",
    "title": "Linear Regression from Scratch",
    "section": "It’s Just a Regression With More Variables!",
    "text": "It’s Just a Regression With More Variables!\n\\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_n X_n + \\epsilon\\]"
  },
  {
    "objectID": "index.html#what-we-havent-covered",
    "href": "index.html#what-we-havent-covered",
    "title": "Linear Regression from Scratch",
    "section": "What We Haven’t Covered",
    "text": "What We Haven’t Covered\n\nModel interpretation - Reading regression tables, understanding coefficients\nModel evaluation - \\(R^2\\), Root Mean Squared Error (RMSE)\nAssumptions of linear regression - Linearity, independence, homoscedasticity, no multicollinearity\nModel diagnostics"
  },
  {
    "objectID": "index.html#the-magic-was-within-you-all-along",
    "href": "index.html#the-magic-was-within-you-all-along",
    "title": "Linear Regression from Scratch",
    "section": "The Magic Was Within You All Along",
    "text": "The Magic Was Within You All Along\n\nAt its heart, regression is relatively simple. It is just fitting a line of best fit to your data.\nThe component parts of a regression model fit together to estimate the explained variance (the intercept and slope), and the unexplained variance (the error term). This allows us to make predictions and quantify the uncertainty of those predictions.\nLinear regression is not magic (even if it often feels that way).\n\nThe magic comes from good theory, good data, and attention to detail (but mostly good theory)."
  },
  {
    "objectID": "index.html#further-resources",
    "href": "index.html#further-resources",
    "title": "Linear Regression from Scratch",
    "section": "Further Resources",
    "text": "Further Resources\n\nMLU-Explain - Linear Regression\nJeremy Balka (jbstatistics) - Simple Linear Regression\nJosh Starmer (StatQuest) - Linear Regression and Linear Models\nJeremy Balka (jbstatistics) - Deriving the Least Squares Estimators of the Slope and Intercept\nAndrew Gelman, Jennifer Hill, & Aki Vehtari - Regression & Other Stories"
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Linear Regression from Scratch",
    "section": "References",
    "text": "References\n\n\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2021. Regression and Other Stories. Cambridge University Press.\n\n\nRowntree, Derek. 2018. Statistics Without Tears: An Introduction for Non-Mathematicians. Penguin Books."
  }
]